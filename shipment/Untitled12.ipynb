{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mMnqAtmbxy0",
        "outputId": "b9ed78c8-6659-42e1-f216-549482532c37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Define data paths for real data\n",
        "data_paths = {\n",
        "    \"stationary\": {\"path\": r\"/content/drive/MyDrive/smart_shipment/0 stationary\", \"target\": 0},\n",
        "    \"moving\":     {\"path\": r\"/content/drive/MyDrive/smart_shipment/1 moving\", \"target\": 1},\n",
        "    \"picked\":     {\"path\": r\"/content/drive/MyDrive/smart_shipment/2 picked\", \"target\": 2},\n",
        "    \"wrong\":      {\"path\": r\"/content/drive/MyDrive/smart_shipment/3 wrong\", \"target\": 3},\n",
        "    \"thrown\":     {\"path\": r\"/content/drive/MyDrive/smart_shipment/4 thrown\", \"target\": 4}\n",
        "}\n",
        "\n",
        "# Parameters for windowing (from synthetic generation approach)\n",
        "window_size = 50\n",
        "sensor_cols = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']  # Correct sensor columns\n",
        "\n",
        "# Collect features and labels from real data with window-based feature extraction\n",
        "data = []\n",
        "labels = []\n",
        "all_data_dfs = []  # For optional combined CSV saving\n",
        "\n",
        "for label_name, info in data_paths.items():\n",
        "    files = glob.glob(os.path.join(info[\"path\"], \"*.csv\"))\n",
        "    for file in files:\n",
        "        df = pd.read_csv(file)\n",
        "        # Add target column for optional saving\n",
        "        df[\"target\"] = info[\"target\"]\n",
        "        all_data_dfs.append(df)\n",
        "\n",
        "        # Select sensor data\n",
        "        if not all(col in df.columns for col in sensor_cols):\n",
        "            print(f\"Skipping {file}: Missing sensor columns.\")\n",
        "            continue\n",
        "        data_df = df[sensor_cols]\n",
        "\n",
        "        # Convert to numeric and drop NaNs\n",
        "        data_df = data_df.apply(pd.to_numeric, errors='coerce')\n",
        "        data_df = data_df.dropna()\n",
        "\n",
        "        # Extract windows\n",
        "        num_windows = len(data_df) // window_size\n",
        "        for i in range(num_windows):\n",
        "            window = data_df.iloc[i * window_size : (i + 1) * window_size]\n",
        "            acc = window[['acc_x', 'acc_y', 'acc_z']].values\n",
        "            gyro = window[['gyro_x', 'gyro_y', 'gyro_z']].values\n",
        "            features = np.concatenate([\n",
        "                acc.mean(axis=0), acc.std(axis=0), gyro.mean(axis=0), gyro.std(axis=0)\n",
        "            ])\n",
        "            data.append(features)\n",
        "            labels.append(info[\"target\"])\n",
        "\n",
        "# Convert to arrays\n",
        "X = np.array(data)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Optional: Save combined raw dataset (before windowing)\n",
        "combined_df = pd.concat(all_data_dfs, ignore_index=True)\n",
        "combined_df.to_csv(\"combined_dataset.csv\", index=False)\n",
        "print(f\"Combined dataset saved. Shape: {combined_df.shape}\")\n",
        "\n",
        "# If no data was collected (e.g., insufficient windows), fallback to synthetic\n",
        "if len(X) == 0:\n",
        "    print(\"No real data windows found. Generating synthetic data.\")\n",
        "    def generate_synthetic_data(num_samples_per_class=500, num_classes=5, window_size=50):\n",
        "        data = []\n",
        "        labels = []\n",
        "\n",
        "        for class_id in range(num_classes):\n",
        "            for _ in range(num_samples_per_class // window_size):\n",
        "                if class_id == 0:  # stationary: small noise around gravity\n",
        "                    acc = np.random.normal([0, 0, 1], 0.01, size=(window_size, 3))\n",
        "                    gyro = np.random.normal(0, 0.1, size=(window_size, 3))\n",
        "                elif class_id == 1:  # moving: sinusoidal motion\n",
        "                    t = np.linspace(0, 2 * np.pi, window_size)\n",
        "                    acc = np.column_stack([np.sin(t)*0.5, np.cos(t)*0.3, np.ones(window_size) + np.sin(t)*0.1])\n",
        "                    gyro = np.column_stack([np.cos(t)*5, np.sin(t)*3, np.random.normal(0, 1, window_size)])\n",
        "                elif class_id == 2:  # picked: sudden upward acceleration\n",
        "                    acc = np.column_stack([np.random.normal(0, 0.05, window_size),\n",
        "                                           np.random.normal(0, 0.05, window_size),\n",
        "                                           np.linspace(1, 1.5, window_size) + np.random.normal(0, 0.1, window_size)])\n",
        "                    gyro = np.random.normal(0, 2, size=(window_size, 3))\n",
        "                elif class_id == 3:  # wrong position: tilted\n",
        "                    acc = np.random.normal([0.5, 0.5, 0.7], 0.05, size=(window_size, 3))\n",
        "                    gyro = np.random.normal([10, 10, 0], 1, size=(window_size, 3))\n",
        "                elif class_id == 4:  # thrown: high acceleration and rotation\n",
        "                    acc = np.random.normal(0, 1, size=(window_size, 3)) + np.linspace(0, 2, window_size)[:, None]\n",
        "                    gyro = np.random.normal(0, 20, size=(window_size, 3))\n",
        "\n",
        "                # Flatten the window into features\n",
        "                features = np.concatenate([\n",
        "                    acc.mean(axis=0), acc.std(axis=0), gyro.mean(axis=0), gyro.std(axis=0)\n",
        "                ])\n",
        "                data.append(features)\n",
        "                labels.append(class_id)\n",
        "\n",
        "        return np.array(data), np.array(labels)\n",
        "\n",
        "    X, y = generate_synthetic_data()\n",
        "\n",
        "# Preprocess: Scale features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Dataset\n",
        "class SensorDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# DataLoaders\n",
        "train_dataset = SensorDataset(X_train, y_train)\n",
        "test_dataset = SensorDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Simple MLP Model\n",
        "class ShipmentClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(ShipmentClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "input_size = X.shape[1]  # 12 features (mean/std for 6 sensors)\n",
        "num_classes = 5\n",
        "model = ShipmentClassifier(input_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_pred.extend(predicted.numpy())\n",
        "        y_true.extend(labels.numpy())\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'shipment_model.pkl')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Evaluate\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_pred.extend(predicted.numpy())\n",
        "        y_true.extend(labels.numpy())\n",
        "\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "new_df = pd.read_csv(\"/content/drive/MyDrive/test_1/THROWN.csv\")\n",
        "\n",
        "# Preprocess the same way as training data\n",
        "# Drop target column if it exists\n",
        "if 'target' in new_df.columns:\n",
        "    new_df = new_df.drop(columns=['target'], errors='ignore')\n",
        "\n",
        "# Select only sensor columns and convert to numeric, dropping NaNs\n",
        "new_df = new_df[sensor_cols].apply(pd.to_numeric, errors='coerce').dropna()\n",
        "\n",
        "# Apply window-based feature extraction to new data\n",
        "new_data = []\n",
        "num_windows = len(new_df) // window_size\n",
        "\n",
        "for i in range(num_windows):\n",
        "    window = new_df.iloc[i * window_size : (i + 1) * window_size]\n",
        "    acc = window[['acc_x', 'acc_y', 'acc_z']].values\n",
        "    gyro = window[['gyro_x', 'gyro_y', 'gyro_z']].values\n",
        "    features = np.concatenate([\n",
        "        acc.mean(axis=0), acc.std(axis=0), gyro.mean(axis=0), gyro.std(axis=0)\n",
        "    ])\n",
        "    new_data.append(features)\n",
        "\n",
        "# Convert to array\n",
        "new_data_array = np.array(new_data)\n",
        "\n",
        "\n",
        "# Scale the new data using the same scaler fitted on the training data\n",
        "# This assumes the scaler object is still available from the previous cell\n",
        "if 'scaler' in globals():\n",
        "    new_data_scaled = scaler.transform(new_data_array)\n",
        "    new_data_scaled_tensor = torch.tensor(new_data_scaled, dtype=torch.float32)\n",
        "else:\n",
        "    print(\"Scaler not found. Cannot scale new data. Using unscaled data.\")\n",
        "    new_data_scaled_tensor = torch.tensor(new_data_array, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(new_data_scaled_tensor)\n",
        "    _, predicted_classes = torch.max(outputs, 1)\n",
        "\n",
        "# The predicted_classes tensor contains the predicted class index for each window of the new data\n",
        "print(\"Predicted classes:\", predicted_classes.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuZhlmDVb2JQ",
        "outputId": "f856df4e-2fc2-400a-a02f-880a554b420b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined dataset saved. Shape: (16033, 7)\n",
            "Epoch [10/50], Loss: 0.7826\n",
            "Epoch [20/50], Loss: 0.2970\n",
            "Epoch [30/50], Loss: 0.0329\n",
            "Epoch [40/50], Loss: 0.0369\n",
            "Epoch [50/50], Loss: 0.0380\n",
            "Accuracy: 96.61%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96        11\n",
            "           1       0.93      1.00      0.96        13\n",
            "           2       1.00      0.85      0.92        13\n",
            "           3       1.00      1.00      1.00         9\n",
            "           4       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           0.97        59\n",
            "   macro avg       0.97      0.97      0.97        59\n",
            "weighted avg       0.97      0.97      0.97        59\n",
            "\n",
            "Predicted classes: [4 4 4 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this import at the top of your training script\n",
        "import joblib\n",
        "\n",
        "# In your training code, after this line:\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Add this line to save the scaler:\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "print(\"Scaler saved as scaler.pkl\")"
      ],
      "metadata": {
        "id": "0-bmaT6_V82r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963ff1ed-f8f9-4613-f5de-8c61ab30cb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaler saved as scaler.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this after loading the model\n",
        "test_features = np.random.randn(1, 12)  # Replace with actual features from your training data\n",
        "test_features_scaled = scaler.transform(test_features)\n",
        "test_tensor = torch.tensor(test_features_scaled, dtype=torch.float32)\n",
        "with torch.no_grad():\n",
        "    outputs = model(test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "print(f\"Test prediction: {predicted.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGwBh0hgYdDm",
        "outputId": "b6dadf83-1d2a-4b54-8893-247d5e361f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test prediction: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (your existing training code up to the scaler and model training)\n",
        "\n",
        "# After scaler.fit_transform(X)\n",
        "import joblib\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "# When saving the model\n",
        "torch.save(model.state_dict(), 'shipment_model.pkl', pickle_protocol=2)\n",
        "\n",
        "# To download both files\n",
        "from google.colab import files\n",
        "files.download('shipment_model.pkl')\n",
        "files.download('scaler.pkl')"
      ],
      "metadata": {
        "id": "z4TqmCr8gNN4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6af69f15-f08d-421d-d855-3a6ec119b15c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8fa4c237-b23f-418d-aed3-4278c510919a\", \"shipment_model.pkl\", 15337)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fa968941-9cc6-4607-a9cd-db800b304e44\", \"scaler.pkl\", 903)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Untitled12.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1GhbN3b0BNqfLYCjNwreoAaVYgOXfB8pd\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import glob\n",
        "import os\n",
        "import joblib  # For saving the scaler\n",
        "\n",
        "# Define data paths for real data\n",
        "data_paths = {\n",
        "    \"stationary\": {\"path\": r\"/content/drive/MyDrive/smart_shipment/0 stationary\", \"target\": 0},\n",
        "    \"moving\":     {\"path\": r\"/content/drive/MyDrive/smart_shipment/1 moving\", \"target\": 1},\n",
        "    \"picked\":     {\"path\": r\"/content/drive/MyDrive/smart_shipment/2 picked\", \"target\": 2},\n",
        "    \"wrong\":      {\"path\": r\"/content/drive/MyDrive/smart_shipment/3 wrong\", \"target\": 3},\n",
        "    \"thrown\":     {\"path\": r\"/content/drive/MyDrive/smart_shipment/4 thrown\", \"target\": 4}\n",
        "}\n",
        "\n",
        "# Parameters for windowing\n",
        "window_size = 50\n",
        "sensor_cols = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n",
        "\n",
        "# Collect features and labels from real data with window-based feature extraction\n",
        "data = []\n",
        "labels = []\n",
        "all_data_dfs = []  # For optional combined CSV saving\n",
        "\n",
        "for label_name, info in data_paths.items():\n",
        "    files = glob.glob(os.path.join(info[\"path\"], \"*.csv\"))\n",
        "    for file in files:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            # Add target column for optional saving\n",
        "            df[\"target\"] = info[\"target\"]\n",
        "            all_data_dfs.append(df)\n",
        "\n",
        "            # Select sensor data\n",
        "            if not all(col in df.columns for col in sensor_cols):\n",
        "                print(f\"Skipping {file}: Missing sensor columns.\")\n",
        "                continue\n",
        "            data_df = df[sensor_cols]\n",
        "\n",
        "            # Convert to numeric and drop NaNs\n",
        "            data_df = data_df.apply(pd.to_numeric, errors='coerce')\n",
        "            data_df = data_df.dropna()\n",
        "\n",
        "            # Extract windows\n",
        "            num_windows = len(data_df) // window_size\n",
        "            for i in range(num_windows):\n",
        "                window = data_df.iloc[i * window_size : (i + 1) * window_size]\n",
        "                acc = window[['acc_x', 'acc_y', 'acc_z']].values\n",
        "                gyro = window[['gyro_x', 'gyro_y', 'gyro_z']].values\n",
        "                features = np.concatenate([\n",
        "                    acc.mean(axis=0), acc.std(axis=0), gyro.mean(axis=0), gyro.std(axis=0)\n",
        "                ])\n",
        "                data.append(features)\n",
        "                labels.append(info[\"target\"])\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "# Convert to arrays\n",
        "X = np.array(data)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Optional: Save combined raw dataset (before windowing)\n",
        "if all_data_dfs:\n",
        "    combined_df = pd.concat(all_data_dfs, ignore_index=True)\n",
        "    combined_df.to_csv(\"combined_dataset.csv\", index=False)\n",
        "    print(f\"Combined dataset saved. Shape: {combined_df.shape}\")\n",
        "\n",
        "# If no data was collected (e.g., insufficient windows), fallback to synthetic\n",
        "if len(X) == 0:\n",
        "    print(\"No real data windows found. Generating synthetic data.\")\n",
        "    def generate_synthetic_data(num_samples_per_class=500, num_classes=5, window_size=50):\n",
        "        data = []\n",
        "        labels = []\n",
        "\n",
        "        for class_id in range(num_classes):\n",
        "            for _ in range(num_samples_per_class // window_size):\n",
        "                if class_id == 0:  # stationary: small noise around gravity\n",
        "                    acc = np.random.normal([0, 0, 1], 0.01, size=(window_size, 3))\n",
        "                    gyro = np.random.normal(0, 0.1, size=(window_size, 3))\n",
        "                elif class_id == 1:  # moving: sinusoidal motion\n",
        "                    t = np.linspace(0, 2 * np.pi, window_size)\n",
        "                    acc = np.column_stack([np.sin(t)*0.5, np.cos(t)*0.3, np.ones(window_size) + np.sin(t)*0.1])\n",
        "                    gyro = np.column_stack([np.cos(t)*5, np.sin(t)*3, np.random.normal(0, 1, window_size)])\n",
        "                elif class_id == 2:  # picked: sudden upward acceleration\n",
        "                    acc = np.column_stack([np.random.normal(0, 0.05, window_size),\n",
        "                                           np.random.normal(0, 0.05, window_size),\n",
        "                                           np.linspace(1, 1.5, window_size) + np.random.normal(0, 0.1, window_size)])\n",
        "                    gyro = np.random.normal(0, 2, size=(window_size, 3))\n",
        "                elif class_id == 3:  # wrong position: tilted\n",
        "                    acc = np.random.normal([0.5, 0.5, 0.7], 0.05, size=(window_size, 3))\n",
        "                    gyro = np.random.normal([10, 10, 0], 1, size=(window_size, 3))\n",
        "                elif class_id == 4:  # thrown: high acceleration and rotation\n",
        "                    acc = np.random.normal(0, 1, size=(window_size, 3)) + np.linspace(0, 2, window_size)[:, None]\n",
        "                    gyro = np.random.normal(0, 20, size=(window_size, 3))\n",
        "\n",
        "                # Flatten the window into features\n",
        "                features = np.concatenate([\n",
        "                    acc.mean(axis=0), acc.std(axis=0), gyro.mean(axis=0), gyro.std(axis=0)\n",
        "                ])\n",
        "                data.append(features)\n",
        "                labels.append(class_id)\n",
        "\n",
        "        return np.array(data), np.array(labels)\n",
        "\n",
        "    X, y = generate_synthetic_data()\n",
        "\n",
        "# Preprocess: Scale features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "print(\"Scaler saved as scaler.pkl\")\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Dataset\n",
        "class SensorDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# DataLoaders\n",
        "train_dataset = SensorDataset(X_train, y_train)\n",
        "test_dataset = SensorDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Simple MLP Model\n",
        "class ShipmentClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(ShipmentClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "input_size = X.shape[1]  # 12 features (mean/std for 6 sensors)\n",
        "num_classes = 5\n",
        "model = ShipmentClassifier(input_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_pred.extend(predicted.numpy())\n",
        "        y_true.extend(labels.numpy())\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Save the model with .pth extension and compatible protocol\n",
        "torch.save(model.state_dict(), 'shipment_model.pth', pickle_protocol=2)\n",
        "print(\"Model saved as shipment_model.pth\")\n",
        "\n",
        "# To download both files in Colab\n",
        "from google.colab import files\n",
        "files.download('shipment_model.pth')\n",
        "files.download('scaler.pkl')\n",
        "\n",
        "# ... (your new data prediction code remains unchanged, but update the load line below)"
      ],
      "metadata": {
        "id": "oO-T5DBZZe6L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "ada0865a-8a1e-4933-82c7-a319031c84d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Combined dataset saved. Shape: (16033, 7)\n",
            "Scaler saved as scaler.pkl\n",
            "Epoch [10/50], Loss: 0.5040\n",
            "Epoch [20/50], Loss: 0.2246\n",
            "Epoch [30/50], Loss: 0.0139\n",
            "Epoch [40/50], Loss: 0.0299\n",
            "Epoch [50/50], Loss: 0.0322\n",
            "Accuracy: 98.31%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96        11\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      0.92      0.96        13\n",
            "           3       1.00      1.00      1.00         9\n",
            "           4       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           0.98        59\n",
            "   macro avg       0.98      0.98      0.98        59\n",
            "weighted avg       0.98      0.98      0.98        59\n",
            "\n",
            "Model saved as shipment_model.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5a41d795-4b74-47c2-b977-622b802e277d\", \"shipment_model.pth\", 15337)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a8104d5a-3043-4869-9e16-1a71950e6fed\", \"scaler.pkl\", 903)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4smrOikhtrI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}